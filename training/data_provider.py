import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib  # ƒê·ªÉ l∆∞u l·∫°i b·ªô Scaler d√πng cho sau n√†y
import os

class DataProvider:
    def __init__(self, data_path=None, window_size=60):
        if data_path is None:
            current_file_dir = os.path.dirname(os.path.abspath(__file__))

            # C√°ch 1: Th·ª≠ l√πi 1 c·∫•p (Gi·∫£ s·ª≠ c·∫•u tr√∫c l√† Gold/training/file.py)
            path_option_1 = os.path.join(os.path.dirname(current_file_dir), "data", "processed", "Master_Dataset.csv")

            # C√°ch 2: Th·ª≠ l√πi 2 c·∫•p (Gi·∫£ s·ª≠ c·∫•u tr√∫c l√† Gold/src/training/file.py)
            path_option_2 = os.path.join(os.path.dirname(os.path.dirname(current_file_dir)), "data", "processed",
                                         "Master_Dataset.csv")

            # Ki·ªÉm tra xem c√°i n√†o ƒë√∫ng
            if os.path.exists(path_option_1):
                self.data_path = path_option_1
            elif os.path.exists(path_option_2):
                self.data_path = path_option_2
            else:
                # N·∫øu kh√¥ng t√¨m th·∫•y c·∫£ 2, m·∫∑c ƒë·ªãnh d√πng option 1 ƒë·ªÉ b√°o l·ªói cho d·ªÖ hi·ªÉu
                self.data_path = path_option_1
        else:
            self.data_path = data_path

        print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {self.data_path}")

        # Ki·ªÉm tra l·∫ßn cu·ªëi
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(
                f"‚ùå V·∫´n kh√¥ng t√¨m th·∫•y file! B·∫°n h√£y ki·ªÉm tra xem file 'Master_Dataset.csv' c√≥ n·∫±m trong th∆∞ m·ª•c 'data/processed' kh√¥ng?")
        self.window_size = window_size

        # ƒê·ªãnh nghƒ©a c√°c c·ªôt s·∫Ω d√πng cho t·ª´ng nh√°nh
        # Nh√°nh 1: LSTM (Technical)
        self.tech_cols = ['Gold_Close', 'Log_Return', 'RSI', 'Volatility_20d', 'Trend_Signal']

        # Nh√°nh 2: Dense (Macro) - L∆∞u √Ω t√™n c·ªôt ph·∫£i kh·ªõp v·ªõi file CSV c·ªßa b·∫°n
        # N·∫øu file CSV d√πng t√™n kh√°c (v√≠ d·ª• US10Y thay v√¨ DGS10), h√£y s·ª≠a ·ªü ƒë√¢y
        self.macro_cols = ['DXY', 'US10Y', 'CPI', 'Real_Rate']

        # Target (Output)
        self.target_cols = ['Target_Min_Change', 'Target_Max_Change']

        # Scaler
        self.scaler_tech = MinMaxScaler()
        self.scaler_macro = MinMaxScaler()

    def load_and_split(self, train_ratio=0.8):
        """
        H√†m ch√≠nh ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu train/test
        """
        # 1. Load d·ªØ li·ªáu
        df = pd.read_csv(self.data_path, index_col=0, parse_dates=True)

        # Ki·ªÉm tra xem c√°c c·ªôt macro c√≥ t·ªìn t·∫°i kh√¥ng, n·∫øu thi·∫øu th√¨ b·ªè qua ho·∫∑c b√°o l·ªói
        available_macro = [c for c in self.macro_cols if c in df.columns]
        if len(available_macro) < len(self.macro_cols):
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Thi·∫øu c·ªôt Macro. T√¨m th·∫•y: {available_macro}")
            self.macro_cols = available_macro

        # 2. Chu·∫©n h√≥a (Scaling) - C·ª±c k·ª≥ quan tr·ªçng cho LSTM
        # Fit scaler tr√™n to√†n b·ªô data (ho·∫∑c ch·ªâ train set ƒë·ªÉ chu·∫©n x√°c h∆°n, nh∆∞ng ·ªü ƒë√¢y l√†m ƒë∆°n gi·∫£n tr∆∞·ªõc)
        data_tech_scaled = self.scaler_tech.fit_transform(df[self.tech_cols])
        data_macro_scaled = self.scaler_macro.fit_transform(df[self.macro_cols])
        targets = df[self.target_cols].values  # Target % change th∆∞·ªùng nh·ªè n√™n kh√¥ng c·∫ßn scale, ho·∫∑c scale t√πy √Ω

        # 3. T·∫°o Sliding Window (C·∫Øt l√°t d·ªØ li·ªáu)
        X_tech, X_macro, y = [], [], []

        # Ch·∫°y t·ª´ ng√†y th·ª© 60 ƒë·∫øn h·∫øt
        for i in range(self.window_size, len(df)):
            # Input A: L·∫•y 60 ng√†y qu√° kh·ª© c·ªßa c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t
            tech_window = data_tech_scaled[i - self.window_size:i]

            # Input B: L·∫•y gi√° tr·ªã vƒ© m√¥ c·ªßa ng√†y hi·ªán t·∫°i (ng√†y th·ª© i)
            # L√Ω do: Ta mu·ªën bi·∫øt vƒ© m√¥ H√îM NAY ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn t∆∞∆°ng lai
            macro_current = data_macro_scaled[i - 1]

            # Output: Target c·ªßa d√≤ng hi·ªán t·∫°i
            target_current = targets[i]

            X_tech.append(tech_window)
            X_macro.append(macro_current)
            y.append(target_current)

        # Chuy·ªÉn sang Numpy Array
        X_tech = np.array(X_tech)
        X_macro = np.array(X_macro)
        y = np.array(y)

        # 4. Chia Train / Test
        split_idx = int(len(X_tech) * train_ratio)

        X_train = {
            'input_price': X_tech[:split_idx],
            'input_macro': X_macro[:split_idx]
        }
        y_train = {
            'output_min': y[:split_idx, 0],
            'output_max': y[:split_idx, 1]
        }

        X_test = {
            'input_price': X_tech[split_idx:],
            'input_macro': X_macro[split_idx:]
        }
        y_test = {
            'output_min': y[split_idx:, 0],
            'output_max': y[split_idx:, 1]
        }

        print(f"‚úÖ ƒê√£ chu·∫©n b·ªã d·ªØ li·ªáu xong!")
        print(f"   - Shape X_tech (Train): {X_train['input_price'].shape}")
        print(f"   - Shape X_macro (Train): {X_train['input_macro'].shape}")

        return X_train, y_train, X_test, y_test

    def save_scalers(self, path="src/models/"):
        """L∆∞u scaler ƒë·ªÉ d√πng l√∫c d·ª± ƒëo√°n th·ª±c t·∫ø (Inference)"""
        joblib.dump(self.scaler_tech, f"{path}scaler_tech.pkl")
        joblib.dump(self.scaler_macro, f"{path}scaler_macro.pkl")
        print("üíæ ƒê√£ l∆∞u Scalers.")


# --- Test th·ª≠ ---
if __name__ == "__main__":
    provider = DataProvider(window_size=60)
    try:
        X_train, y_train, X_test, y_test = provider.load_and_split()
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")