import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import matplotlib.pyplot as plt

# Import c√°c class ch√∫ng ta ƒë√£ vi·∫øt
# L∆∞u √Ω: Python hi·ªÉu ƒë∆∞·ªùng d·∫´n t·ª´ th∆∞ m·ª•c g·ªëc d·ª± √°n khi ch·∫°y main_train.py
from training.data_provider import DataProvider
from models.hybrid_model import GoldPriceModel


class GoldTrainer:
    def __init__(self, epochs=50, batch_size=32):
        self.epochs = epochs
        self.batch_size = batch_size
        self.model_save_path = "models/best_gold_model.keras"  # ƒêu√¥i .keras l√† chu·∫©n m·ªõi c·ªßa TensorFlow

        # ƒê·∫£m b·∫£o th∆∞ m·ª•c models t·ªìn t·∫°i
        os.makedirs("models", exist_ok=True)

    def train(self):
        print("üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN...")

        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
        provider = DataProvider(window_size=60)
        X_train, y_train, X_test, y_test = provider.load_and_split()

        # L∆∞u l·∫°i Scaler ƒë·ªÉ sau n√†y d√πng cho d·ª± ƒëo√°n th·ª±c t·∫ø
        provider.save_scalers(path="models/")

        # 2. X√¢y d·ª±ng m√¥ h√¨nh
        # L·∫•y s·ªë l∆∞·ª£ng features ƒë·ªông t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o
        n_features_price = X_train['input_price'].shape[2]  # S·∫Ω l√† 5
        n_features_macro = X_train['input_macro'].shape[1]  # S·∫Ω l√† 4

        print(f"üìä C·∫•u h√¨nh Input: Price Features={n_features_price}, Macro Features={n_features_macro}")

        model_builder = GoldPriceModel(
            window_size=60,
            n_features_price=n_features_price,
            n_features_macro=n_features_macro
        )
        model = model_builder.build_model()

        # 3. Compile M√¥ h√¨nh
        # Loss function: MSE (Mean Squared Error) ƒë·ªÉ t·ªëi ∆∞u h√≥a sai s·ªë b√¨nh ph∆∞∆°ng
        # Metrics: MAE (Mean Absolute Error) ƒë·ªÉ d·ªÖ ƒë·ªçc sai s·ªë th·ª±c t·∫ø
        model.compile(
            optimizer='adam',
            loss={'output_min': 'mse', 'output_max': 'mse'},
            metrics={'output_min': 'mae', 'output_max': 'mae'}
        )

        # 4. C·∫•u h√¨nh Callbacks (Tr·ª£ l√Ω hu·∫•n luy·ªán)
        callbacks = [
            # Ch·ªâ l∆∞u model n·∫øu validation loss gi·∫£m (Model t·ªët l√™n)
            ModelCheckpoint(
                self.model_save_path,
                monitor='val_loss',
                save_best_only=True,
                mode='min',
                verbose=1
            ),
            # D·ª´ng train s·ªõm n·∫øu 10 epochs li√™n ti·∫øp kh√¥ng ti·∫øn b·ªô (tr√°nh t·ªën ƒëi·ªán)
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
        ]

        # 5. B·∫ÆT ƒê·∫¶U TRAIN (FIT)
        history = model.fit(
            x=X_train,
            y=y_train,
            validation_data=(X_test, y_test),
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1
        )

        print("‚úÖ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T!")
        self.plot_history(history)

    def plot_history(self, history):
        """V·∫Ω bi·ªÉu ƒë·ªì Loss ƒë·ªÉ xem AI h·ªçc th·∫ø n√†o"""
        plt.figure(figsize=(12, 5))

        # V·∫Ω Loss t·ªïng
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('T·ªïng Sai S·ªë (Total Loss)')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        # V·∫Ω MAE c·ªßa Min v√† Max
        plt.subplot(1, 2, 2)
        plt.plot(history.history['val_output_min_mae'], label='Sai s·ªë Min (Val)')
        plt.plot(history.history['val_output_max_mae'], label='Sai s·ªë Max (Val)')
        plt.title('Sai s·ªë Tuy·ªát ƒë·ªëi (MAE) tr√™n t·∫≠p Test')
        plt.xlabel('Epochs')
        plt.legend()

        plt.tight_layout()
        plt.show()