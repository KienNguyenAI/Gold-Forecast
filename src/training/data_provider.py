import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib
import os
import logging
from typing import Dict, Tuple


class DataProvider:
    def __init__(self, settings: Dict):
        """
        Kh·ªüi t·∫°o DataProvider v·ªõi c·∫•u h√¨nh.
        :param settings: Config dictionary (settings.yaml)
        """
        self.logger = logging.getLogger(__name__)
        self.settings = settings

        # L·∫•y ƒë∆∞·ªùng d·∫´n t·ª´ config
        processed_dir = settings['paths']['processed_data']
        self.data_path = os.path.join(processed_dir, "gold_processed_features.csv")
        self.model_save_path = settings['paths']['model_save']  # N∆°i l∆∞u scaler

        # L·∫•y tham s·ªë x·ª≠ l√Ω
        self.window_size = settings['processing']['window_size']
        self.test_ratio = settings['processing']['test_size']

        # ƒê·ªãnh nghƒ©a c·ªôt (C√≥ th·ªÉ ƒë∆∞a v√†o config n·∫øu mu·ªën linh ho·∫°t h∆°n n·ªØa)
        self.tech_cols = ['Gold_Close', 'Log_Return', 'RSI', 'Volatility_20d', 'Trend_Signal']
        self.macro_cols = ['DXY', 'US10Y', 'CPI', 'Real_Rate']
        self.target_cols = ['Target_Min_Change', 'Target_Max_Change']

        self.scaler_tech = MinMaxScaler()
        self.scaler_macro = MinMaxScaler()

    def load_and_split(self, for_training=True) -> Tuple[Dict, Dict, Dict, Dict]:
        self.logger.info(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {self.data_path}")

        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i: {self.data_path}")

        df = pd.read_csv(self.data_path, index_col=0, parse_dates=True)

        # Ki·ªÉm tra c·ªôt thi·∫øu
        missing_tech = [c for c in self.tech_cols if c not in df.columns]
        missing_macro = [c for c in self.macro_cols if c not in df.columns]

        if for_training:
            original_len = len(df)
            df = df.dropna(subset=self.target_cols)

        if missing_tech or missing_macro:
            self.logger.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt d·ªØ li·ªáu! Tech: {missing_tech}, Macro: {missing_macro}")
            # ·ªû ƒë√¢y c√≥ th·ªÉ raise error n·∫øu mu·ªën nghi√™m ng·∫∑t

        # Scaling
        # L∆∞u √Ω: N√™n split tr∆∞·ªõc khi scale ƒë·ªÉ tr√°nh data leakage,
        # nh∆∞ng ·ªü b∆∞·ªõc n√†y m√¨nh gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n cho ƒë∆°n gi·∫£n.
        data_tech_scaled = self.scaler_tech.fit_transform(df[self.tech_cols])
        data_macro_scaled = self.scaler_macro.fit_transform(df[self.macro_cols])
        targets = df[self.target_cols].values

        # Sliding Window
        X_tech, X_macro, y = [], [], []

        # Logic t·∫°o window
        for i in range(self.window_size, len(df)):
            tech_window = data_tech_scaled[i - self.window_size:i]
            macro_current = data_macro_scaled[i - 1]  # Macro t·∫°i th·ªùi ƒëi·ªÉm t-1
            target_current = targets[i]

            X_tech.append(tech_window)
            X_macro.append(macro_current)
            y.append(target_current)

        X_tech = np.array(X_tech)
        X_macro = np.array(X_macro)
        y = np.array(y)

        # Split Train/Test
        split_idx = int(len(X_tech) * (1 - self.test_ratio))  # V√≠ d·ª• 0.8

        X_train = {'input_price': X_tech[:split_idx], 'input_macro': X_macro[:split_idx]}
        y_train = {'output_min': y[:split_idx, 0], 'output_max': y[:split_idx, 1]}

        X_test = {'input_price': X_tech[split_idx:], 'input_macro': X_macro[split_idx:]}
        y_test = {'output_min': y[split_idx:, 0], 'output_max': y[split_idx:, 1]}

        self.logger.info(
            f"‚úÖ ƒê√£ split d·ªØ li·ªáu. Train size: {len(X_tech[:split_idx])}, Test size: {len(X_tech[split_idx:])}")

        return X_train, y_train, X_test, y_test

    def save_scalers(self):
        """L∆∞u scaler v√†o artifacts/models/"""
        os.makedirs(self.model_save_path, exist_ok=True)
        joblib.dump(self.scaler_tech, os.path.join(self.model_save_path, "scaler_tech.pkl"))
        joblib.dump(self.scaler_macro, os.path.join(self.model_save_path, "scaler_macro.pkl"))
        self.logger.info(f"üíæ ƒê√£ l∆∞u Scalers v√†o: {self.model_save_path}")