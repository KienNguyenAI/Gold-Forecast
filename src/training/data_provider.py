import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib
import os
import logging
from typing import Dict, Tuple


class DataProvider:
    def __init__(self, settings: Dict):
        """
        Khá»Ÿi táº¡o DataProvider vá»›i cáº¥u hÃ¬nh.
        :param settings: Config dictionary (settings.yaml)
        """
        self.logger = logging.getLogger(__name__)
        self.settings = settings

        # Láº¥y Ä‘Æ°á»ng dáº«n tá»« config
        processed_dir = settings['paths']['processed_data']
        self.data_path = os.path.join(processed_dir, "gold_processed_features.csv")
        self.model_save_path = settings['paths']['model_save']  # NÆ¡i lÆ°u scaler

        # Láº¥y tham sá»‘ xá»­ lÃ½
        self.window_size = settings['processing']['window_size']
        self.test_ratio = settings['processing']['test_size']

        # Äá»‹nh nghÄ©a cá»™t (CÃ³ thá»ƒ Ä‘Æ°a vÃ o config náº¿u muá»‘n linh hoáº¡t hÆ¡n ná»¯a)
        self.tech_cols = ['Gold_Close', 'Log_Return', 'RSI', 'Volatility_20d', 'Trend_Signal']
        self.macro_cols = ['DXY', 'US10Y', 'CPI', 'Real_Rate']
        self.target_cols = ['Target_Min_Change', 'Target_Max_Change']

        self.scaler_tech = MinMaxScaler()
        self.scaler_macro = MinMaxScaler()

    def load_and_split(self) -> Tuple[Dict, Dict, Dict, Dict]:
        self.logger.info(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {self.data_path}")

        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u táº¡i: {self.data_path}")

        df = pd.read_csv(self.data_path, index_col=0, parse_dates=True)

        # Kiá»ƒm tra cá»™t thiáº¿u
        missing_tech = [c for c in self.tech_cols if c not in df.columns]
        missing_macro = [c for c in self.macro_cols if c not in df.columns]

        if missing_tech or missing_macro:
            self.logger.warning(f"âš ï¸ Thiáº¿u cá»™t dá»¯ liá»‡u! Tech: {missing_tech}, Macro: {missing_macro}")
            # á» Ä‘Ã¢y cÃ³ thá»ƒ raise error náº¿u muá»‘n nghiÃªm ngáº·t

        # Scaling
        # LÆ°u Ã½: NÃªn split trÆ°á»›c khi scale Ä‘á»ƒ trÃ¡nh data leakage,
        # nhÆ°ng á»Ÿ bÆ°á»›c nÃ y mÃ¬nh giá»¯ nguyÃªn logic cÅ© cá»§a báº¡n cho Ä‘Æ¡n giáº£n.
        data_tech_scaled = self.scaler_tech.fit_transform(df[self.tech_cols])
        data_macro_scaled = self.scaler_macro.fit_transform(df[self.macro_cols])
        targets = df[self.target_cols].values

        # Sliding Window
        X_tech, X_macro, y = [], [], []

        # Logic táº¡o window
        for i in range(self.window_size, len(df)):
            tech_window = data_tech_scaled[i - self.window_size:i]
            macro_current = data_macro_scaled[i - 1]  # Macro táº¡i thá»i Ä‘iá»ƒm t-1
            target_current = targets[i]

            X_tech.append(tech_window)
            X_macro.append(macro_current)
            y.append(target_current)

        X_tech = np.array(X_tech)
        X_macro = np.array(X_macro)
        y = np.array(y)

        # Split Train/Test
        split_idx = int(len(X_tech) * (1 - self.test_ratio))  # VÃ­ dá»¥ 0.8

        X_train = {'input_price': X_tech[:split_idx], 'input_macro': X_macro[:split_idx]}
        y_train = {'output_min': y[:split_idx, 0], 'output_max': y[:split_idx, 1]}

        X_test = {'input_price': X_tech[split_idx:], 'input_macro': X_macro[split_idx:]}
        y_test = {'output_min': y[split_idx:, 0], 'output_max': y[split_idx:, 1]}

        self.logger.info(
            f"âœ… ÄÃ£ split dá»¯ liá»‡u. Train size: {len(X_tech[:split_idx])}, Test size: {len(X_tech[split_idx:])}")

        return X_train, y_train, X_test, y_test

    def save_scalers(self):
        """LÆ°u scaler vÃ o artifacts/models/"""
        os.makedirs(self.model_save_path, exist_ok=True)
        joblib.dump(self.scaler_tech, os.path.join(self.model_save_path, "scaler_tech.pkl"))
        joblib.dump(self.scaler_macro, os.path.join(self.model_save_path, "scaler_macro.pkl"))
        self.logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u Scalers vÃ o: {self.model_save_path}")