import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import logging
import matplotlib.pyplot as plt
from typing import Dict

# Import class t·ª´ trong src
from .data_provider import DataProvider
# üëá L∆∞u √Ω: Import t·ª´ module models (Hybrid Model)
from src.models.hybrid_model import GoldPriceModel


class ModelTrainer:
    def __init__(self, settings: Dict):
        self.logger = logging.getLogger(__name__)
        self.settings = settings

        # L·∫•y tham s·ªë training t·ª´ config
        self.train_conf = settings['training']
        self.epochs = self.train_conf['epochs']
        self.batch_size = self.train_conf['batch_size']

        # ƒê∆∞·ªùng d·∫´n l∆∞u model
        save_dir = settings['paths']['model_save']
        os.makedirs(save_dir, exist_ok=True)
        # L∆∞u t√™n model c√≥ version ƒë·ªÉ d·ªÖ qu·∫£n l√Ω
        model_name = settings['model'].get('name', 'model')
        self.model_save_path = os.path.join(save_dir, f"{model_name}_best.keras")
        self.figures_dir = settings['paths']['figures_save']

    def train(self):
        self.logger.info("üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN...")

        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
        provider = DataProvider(self.settings)
        X_train, y_train, X_test, y_test = provider.load_and_split()

        # L∆∞u scaler
        provider.save_scalers()

        # 2. X√¢y d·ª±ng m√¥ h√¨nh
        # L·∫•y shape ƒë·ªông
        n_features_price = X_train['input_price'].shape[2]
        n_features_macro = X_train['input_macro'].shape[1]

        self.logger.info(f"üìä Input Features: Price={n_features_price}, Macro={n_features_macro}")

        # Kh·ªüi t·∫°o Builder (truy·ªÅn setting v√†o)
        builder = GoldPriceModel(self.settings)

        # Build v·ªõi shape c·ª• th·ªÉ
        model = builder.build_model(
            input_shape_price=(X_train['input_price'].shape[1], n_features_price),
            input_shape_macro=(n_features_macro,)
        )

        # 3. Compile
        learning_rate = self.train_conf.get('learning_rate', 0.001)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss={'output_min': 'mse', 'output_max': 'mse'},
            metrics={'output_min': 'mae', 'output_max': 'mae'}
        )

        # 4. Callbacks
        callbacks = [
            ModelCheckpoint(
                self.model_save_path,
                monitor='val_loss',
                save_best_only=True,
                mode='min',
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=self.train_conf.get('patience', 10),
                restore_best_weights=True,
                verbose=1
            )
        ]

        # 5. Training Loop
        history = model.fit(
            x=X_train,
            y=y_train,
            validation_data=(X_test, y_test),
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1
        )

        self.logger.info("‚úÖ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T!")
        self.plot_history(history)

        return self.model_save_path

    def plot_history(self, history):
        """V·∫Ω bi·ªÉu ƒë·ªì v√† l∆∞u v√†o file ·∫£nh thay v√¨ ch·ªâ show"""
        os.makedirs(self.figures_dir, exist_ok=True)

        plt.figure(figsize=(12, 5))

        # Plot 1: Loss
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title('Total Loss')
        plt.legend()

        # Plot 2: MAE
        plt.subplot(1, 2, 2)
        # Ki·ªÉm tra key trong history (v√¨ tf version kh√°c nhau c√≥ th·ªÉ ƒë·ªïi t√™n)
        if 'val_output_min_mae' in history.history:
            plt.plot(history.history['val_output_min_mae'], label='Min MAE')
            plt.plot(history.history['val_output_max_mae'], label='Max MAE')
        else:
            # Fallback cho t√™n key kh√°c
            pass

        plt.title('Validation MAE')
        plt.legend()
        plt.tight_layout()

        # L∆∞u ·∫£nh
        save_path = os.path.join(self.figures_dir, "training_history.png")
        plt.savefig(save_path)
        self.logger.info(f"üìâ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì training t·∫°i: {save_path}")
        plt.close()  # ƒê√≥ng plot ƒë·ªÉ gi·∫£i ph√≥ng mem